Results sorting 32G on 16 nodes w/ 16 reducers.

no symlinking - 63 seconds, 6.30M spills
symlinking w/o in memory merging - 86 seconds, 11.35M spills 
symlinking w/ in memory merging - 83 seconds, 11.35M spills

the in memory merging had no affect. Main issue was when I tried to merge data
from disk into memory it was too big for memory. Lets try to shrink the amount
of data per reducer.

Using 32 reducers on 16 nodes sorting 16G

no symlinking - 34 seconds, 1.63M spills
symlinking w/o in memory merging - 51 seconds, 5.67M spills
symlinking w/ in memory merging - 48 seconds, 4.06M spills

Here we begin to see the positive affects.

I think it may be better to bring in individual files into memory
(like in HADOOP9109_symlinkshuffle_manyreducer_mergeoptimization1) and
let in memory code merge it rather that merge from disk and store into
memory. Perhaps through some additional thread. That's the next patch
attempt.


